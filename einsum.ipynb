{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [Definition](#definition)\n",
    "* [1D: Vectors](#1d)\n",
    "    * [Identity: *(i)*](#1d_identity)\n",
    "    * [Binary operations](#1d_binary)\n",
    "        * [Element-wise product, *(i,i -> i)*](#vec_hadamard)\n",
    "        * [Inner product, *(i,i->i)*](#vec_inner)\n",
    "        * [Outer product, *(i,j->ij)*](#vec_outer)\n",
    "* [2D: Matrices](#2d)\n",
    "    * [Unary operations](#2d_unary)\n",
    "        * [Identity: *(ij)*](#2d_identity)\n",
    "        * [Total sum *('ij -> ')*](#2d_sum)\n",
    "        * [Sum along axis. *('ij -> i')*](#2d_axis_sum)\n",
    "        * [Transposition. *('ij -> ji')*](#2d_transpose)\n",
    "        * [Diagonal. *('ii -> i')*](#2d_diagonal)\n",
    "        * [Trace. *('ii')*](#2d_trace)\n",
    "    * [Binary operations](#2d_unary)\n",
    "        * [Element-wise product. *('ij, ij -> ij')*](#2d_hadamard)\n",
    "        * [Matrix product, *(ij,jk)*](#2d_inner)\n",
    "        * [Expansion. *('ij, kl -> ijk')*](#2d_expansion)\n",
    "        * [Other composite operations. ('ij, jk -> ij')](#2d_other)\n",
    "        \n",
    "* [3D](#3d)\n",
    "    * [Tensor Contraction](#3d_contraction)\n",
    "    * [Batch Matrix multiplication](#3d_batch_mat_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div id='refs'>References:</div>\n",
    "* [Torch einsum](https://pytorch.org/docs/stable/torch.html#torch.einsum)\n",
    "* [Numpy einsum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html)\n",
    "* [basic guide to einsum](https://ajcr.net/Basic-guide-to-einsum/)\n",
    "* [Einstein Summation in Numpy](https://obilaniu6266h16.wordpress.com/page/3/)\n",
    "* [Einstein Summation, WolframMathWorld](https://mathworld.wolfram.com/EinsteinSummation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id='intro'>Brief intro</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Einstein notation*, AKA *Enstein summation* or *Einsum* is a concise way to express matrix operations. It is particularly suited for tensors operations and, thus, mainly used in Physics. In particular Einstein invented such notation in order to easily and cleary indicate the indexes of the tensors over which to perform matrix products.  \n",
    "[Numpy einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) was the first python implementation of such notation, as much as I know. You can know more from [here](https://ajcr.net/Basic-guide-to-einsum/#historical-notes-and-links).  \n",
    "It is important to notice that **Numpy einsum is not a perfect copy of the Einstein notation**, it has its pros and cons.\n",
    "[Torch einsum](https://pytorch.org/docs/stable/generated/torch.einsum.html#torch-einsum) is the Pytorch version of the Numpy implementation. It came with perks like parallelization on GPU, so it was faster than the Numpy one at the beginning. As much as I know, Numpy recently optimized einsum. Pytorch einsum is not the perfect copy of Numpy einsum. For example I noticed it lacks the broadcasting of a scalar over a vector. \n",
    "\n",
    "For more info you can have a look at the [References](#refs) or Google will help you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id='definition'>Definition</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.einsum(equation, *operands) → Tensor```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.\n",
    "\n",
    ">The equation is given in terms of lower case letters (indices) to be associated with each dimension of the operands and result. The left hand side lists the operands dimensions, separated by commas.\n",
    "* There should be one index letter per tensor dimension. The right hand side follows after -> and gives the indices for the output. If the -> and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side. * The indices not apprearing in the output are summed over after multiplying the operands entries. \n",
    "* If an index appears several times for the same operand, a diagonal is taken.\n",
    "* Ellipses … represent a fixed number of dimensions. If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.332354Z",
     "start_time": "2021-01-04T17:25:26.330054Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:22:23.703511Z",
     "start_time": "2021-01-06T11:22:23.606897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0., 1., 2.])\n",
      " a.shape:torch.Size([3])\n",
      "B:\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      " B.shape:torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([0, 1, 2])\n",
    "\n",
    "B = torch.Tensor([[ 0,  1,  2,  3],\n",
    "              [ 4,  5,  6,  7],\n",
    "              [ 8,  9, 10, 11]])\n",
    "\n",
    "print(f\"a:\\n{a}\\n a.shape:{a.shape}\")\n",
    "print(f\"B:\\n{B}\\n B.shape:{B.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.364432Z",
     "start_time": "2021-01-04T17:25:26.359563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\n",
      "tensor([ 0., 22., 76.])\n",
      " C.shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "C = torch.einsum('i,ij->i', a, B)\n",
    "print(f\"\\nC:\\n{C}\\n C.shape: {C.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:22:30.037762Z",
     "start_time": "2021-01-06T11:22:30.019817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\n",
      "tensor([ 0., 22., 76.])\n",
      " C.shape:\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "C = (a[:, None] * B).sum(axis=1) # product + sum\n",
    "print(f\"\\nC:\\n{C}\\n C.shape:\\n{C.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einsum is basically a product + sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <div id='1d'>1D Tensor: Vector</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.468853Z",
     "start_time": "2021-01-04T17:25:26.464624Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "u = torch.Tensor([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='1d_identity'>Identity: *(i)*</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:23:37.565851Z",
     "start_time": "2021-01-06T11:23:37.549508Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i', u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### <div id='1d_binary'>Binary operations</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.487418Z",
     "start_time": "2021-01-04T17:25:26.483279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v = torch.Tensor([4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='vec_inner'>Inner product</div> *(i,i->i)*, assumes two vectors of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:26:08.705833Z",
     "start_time": "2021-01-04T17:26:08.701666Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i,i', u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.511232Z",
     "start_time": "2021-01-04T17:25:26.502718Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(u*v).sum() # it comes from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.522279Z",
     "start_time": "2021-01-04T17:25:26.514548Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u@v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='vec_hadamard'>dot Element-wise product</div> *(i,i -> i)*, assumes two vectors of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.535236Z",
     "start_time": "2021-01-04T17:25:26.525051Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  5., 12., 21.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i,i->i', u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.546569Z",
     "start_time": "2021-01-04T17:25:26.538711Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  5., 12., 21.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u*v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='vec_outer'>Outer product</div> *(i,j)*, assumes two vectors not necessarily of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:31:19.673996Z",
     "start_time": "2021-01-06T11:31:19.668902Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8., 10., 12., 14.],\n",
       "        [12., 15., 18., 21.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i,j -> ij', u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:31:59.209218Z",
     "start_time": "2021-01-06T11:31:59.202186Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8., 10., 12., 14.],\n",
       "        [12., 15., 18., 21.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[:, None]@v[None,:] # = u(i,1) x v(1,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id='2d'>2D Tensor: Matrix</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:33:09.075556Z",
     "start_time": "2021-01-06T11:33:09.071792Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape:torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([\n",
    "        [ 0,  1,  2,  3],\n",
    "        [ 4,  5,  6,  7],\n",
    "        [ 8,  9, 10, 11]\n",
    "])\n",
    "print(f\"A.shape:{A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say *i* and *j* being rows and columns axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### <div id='2d_unary'>Unary Operations</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='2d_identity'>Identity. *('ij')* </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.579926Z",
     "start_time": "2021-01-04T17:25:26.572791Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.597330Z",
     "start_time": "2021-01-04T17:25:26.591014Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij -> ij', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='2d_sum'>Total sum *('ij -> ')*</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:39:04.504292Z",
     "start_time": "2021-01-06T14:39:04.500012Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij-> ', A) # a scalar output is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:39:13.311721Z",
     "start_time": "2021-01-06T14:39:13.301529Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='2d_axis_sum'>Sum along axis. *('ij -> i')*</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.646583Z",
     "start_time": "2021-01-04T17:25:26.640103Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18., 21.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->j', A) # contraction along i: j (columns) survives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.625756Z",
     "start_time": "2021-01-04T17:25:26.608507Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18., 21.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0) # sum along columns; a row will result: <<4 columns>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.636428Z",
     "start_time": "2021-01-04T17:25:26.629148Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the other axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.668042Z",
     "start_time": "2021-01-04T17:25:26.662525Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 22., 38.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->i', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.659603Z",
     "start_time": "2021-01-04T17:25:26.650311Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 22., 38.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:51:40.124747Z",
     "start_time": "2021-01-03T21:51:40.116422Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='2d_transpose'>Transposition. *('ij -> ji')*</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.676690Z",
     "start_time": "2021-01-04T17:25:26.671117Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.,  8.],\n",
       "        [ 1.,  5.,  9.],\n",
       "        [ 2.,  6., 10.],\n",
       "        [ 3.,  7., 11.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ji', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.687305Z",
     "start_time": "2021-01-04T17:25:26.680130Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.,  8.],\n",
       "        [ 1.,  5.,  9.],\n",
       "        [ 2.,  6., 10.],\n",
       "        [ 3.,  7., 11.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij->ji', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='2d_diagonal'>Diagonal. *('ii -> i')*</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T22:04:14.818873Z",
     "start_time": "2021-01-03T22:04:14.815329Z"
    },
    "hidden": true
   },
   "source": [
    "Needs a trunkated tensor, namely a square one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.705817Z",
     "start_time": "2021-01-04T17:25:26.691181Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dimension does not match previous size, operand 0, dim 1\n"
     ]
    }
   ],
   "source": [
    "try: torch.einsum('ii->i', A)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:37:11.970104Z",
     "start_time": "2021-01-04T17:37:11.965487Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  5., 10.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ii->i', A[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.723920Z",
     "start_time": "2021-01-04T17:25:26.718675Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  5., 10.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.735608Z",
     "start_time": "2021-01-04T17:25:26.726783Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  5., 10.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('jj->j', A[:, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### <div id='2d_trace'>Trace. *('ii')*</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.744169Z",
     "start_time": "2021-01-04T17:25:26.737971Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ii', A[:, :3]) # sum of the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.755876Z",
     "start_time": "2021-01-04T17:25:26.746855Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note the same axis in the declaration, ```(ii)```, different from ```(ij)```, being the identity for 2 rank tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div id='2d_binary'>Binary operations</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:39:59.477743Z",
     "start_time": "2021-01-06T14:39:59.473980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B.shape:torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "B = torch.Tensor([\n",
    "        [ 12,  13,  14,  15],\n",
    "        [ 16,  17, 18, 19],\n",
    "        [ 20, 21, 22, 23]\n",
    "])\n",
    "print(f\"B.shape:{B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### <div id='2d_hadamard'>Element-wise product. *('ij, ij -> ij')*</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Element-wise multiplication of A and B. row vs row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:40:21.955636Z",
     "start_time": "2021-01-06T14:40:21.951959Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([3, 4]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:40:22.204424Z",
     "start_time": "2021-01-06T14:40:22.199560Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,  13.,  28.,  45.],\n",
       "        [ 64.,  85., 108., 133.],\n",
       "        [160., 189., 220., 253.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, ij -> ij', A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Explaination.  \n",
    "Let's take the same operation between their first rows and compare with the previous output. The result will be equal to the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.779703Z",
     "start_time": "2021-01-04T17:25:26.773565Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 13., 28., 45.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, ij -> ij', A[0,None], B[0,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:43:39.683910Z",
     "start_time": "2021-01-06T14:43:39.679673Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 13., 28., 45.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0] * B[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we switch output axes in the einsum we can have the transpose result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:44:18.296669Z",
     "start_time": "2021-01-06T14:44:18.285515Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,  64., 160.],\n",
       "        [ 13.,  85., 189.],\n",
       "        [ 28., 108., 220.],\n",
       "        [ 45., 133., 253.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, ij -> ji', A, B) # just transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div id=''>Matrix product, dot, ('ij,jk -> ik')</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:48:58.407158Z",
     "start_time": "2021-01-06T14:48:58.403693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape:(3, 4), B.shape:(3, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"A.shape:{tuple(A.shape)}, B.shape:{tuple(B.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:48:00.189453Z",
     "start_time": "2021-01-06T14:48:00.184538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 86., 110., 134.],\n",
       "        [302., 390., 478.],\n",
       "        [518., 670., 822.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, jk -> ik', A, B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:48:14.650312Z",
     "start_time": "2021-01-06T14:48:14.645536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 86., 110., 134.],\n",
       "        [302., 390., 478.],\n",
       "        [518., 670., 822.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(A,B.T) # same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic product *(ij, kl)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:45:16.063086Z",
     "start_time": "2021-01-06T16:45:16.058854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, kl -> ik', A, B).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:45:25.360678Z",
     "start_time": "2021-01-06T16:45:25.356698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, kl -> il', A, B).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:53:41.251217Z",
     "start_time": "2021-01-06T16:53:41.247343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, kl -> ij', A, B).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results can be seen as different sums on the dims [expansion](#2d_expansion) (broadcasting) of the two matrices. Below an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:56:06.974521Z",
     "start_time": "2021-01-06T16:56:06.969332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.,  210.,  420.,  630.],\n",
       "        [ 840., 1050., 1260., 1470.],\n",
       "        [1680., 1890., 2100., 2310.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.einsum('ij, kl -> ijkl', A, B) # expansion\n",
    "torch.einsum('ijkl -> ij', a) # sum along k and l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T16:56:28.157154Z",
     "start_time": "2021-01-06T16:56:28.151460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.,  210.,  420.,  630.],\n",
       "        [ 840., 1050., 1260., 1470.],\n",
       "        [1680., 1890., 2100., 2310.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, kl -> ij', A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div id='2d_expansion'>Expansion. *('ij, kl -> ijk')*</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It distributes (broadcast) the product between A and the contraction of B along an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:00:33.123066Z",
     "start_time": "2021-01-06T17:00:33.117925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   0.,   0.],\n",
       "         [ 54.,  70.,  86.],\n",
       "         [108., 140., 172.],\n",
       "         [162., 210., 258.]],\n",
       "\n",
       "        [[216., 280., 344.],\n",
       "         [270., 350., 430.],\n",
       "         [324., 420., 516.],\n",
       "         [378., 490., 602.]],\n",
       "\n",
       "        [[432., 560., 688.],\n",
       "         [486., 630., 774.],\n",
       "         [540., 700., 860.],\n",
       "         [594., 770., 946.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, kl -> ijk', A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.908321Z",
     "start_time": "2021-01-04T17:25:26.901079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.einsum('ij, kl -> ijk', A, B).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T15:05:32.578634Z",
     "start_time": "2021-01-06T15:05:32.573692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([54., 70., 86.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. contraction of B along l\n",
    "B_k = torch.einsum('kl -> k', B)\n",
    "B_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:00:37.818608Z",
     "start_time": "2021-01-06T17:00:37.813023Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   0.,   0.],\n",
       "         [ 54.,  70.,  86.],\n",
       "         [108., 140., 172.],\n",
       "         [162., 210., 258.]],\n",
       "\n",
       "        [[216., 280., 344.],\n",
       "         [270., 350., 430.],\n",
       "         [324., 420., 516.],\n",
       "         [378., 490., 602.]],\n",
       "\n",
       "        [[432., 560., 688.],\n",
       "         [486., 630., 774.],\n",
       "         [540., 700., 860.],\n",
       "         [594., 770., 946.]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. element-wise product between A and B_k\n",
    "torch.einsum('ij, k -> ijk', A, B_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div id='2d_other'>Other composite operations. *('ij, jk -> ij')*</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice the surviving axes: the result will have same dimensions as A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.842706Z",
     "start_time": "2021-01-04T17:25:26.835295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,  51., 108., 171.],\n",
       "        [192., 255., 324., 399.],\n",
       "        [384., 459., 540., 627.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, jk -> ij', A, B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because *i* by *j* matrix A is multiplied against a vector resulting by the contraction of B.T on its first index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.853389Z",
     "start_time": "2021-01-04T17:25:26.845592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 16., 20.],\n",
       "        [13., 17., 21.],\n",
       "        [14., 18., 22.],\n",
       "        [15., 19., 23.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('jk', B.T) # original matrix B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.861310Z",
     "start_time": "2021-01-04T17:25:26.855870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([48., 51., 54., 57.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('jk -> j', B.T) # contraction on j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.877667Z",
     "start_time": "2021-01-04T17:25:26.865361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,  51., 108., 171.],\n",
       "        [192., 255., 324., 399.],\n",
       "        [384., 459., 540., 627.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_T_j = torch.einsum('jk -> j', B.T)\n",
    "torch.einsum('ij, j -> ij', A, B_T_j) # simple product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id='3d'>3D Tensor</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous assumptions can be extended to higher rank tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:07:14.997555Z",
     "start_time": "2021-01-06T17:07:14.993403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape:torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([\n",
    "    [\n",
    "        [ 0,  1,  2,  3],\n",
    "        [ 4,  5,  6,  7],\n",
    "        [ 8,  9, 10, 11]\n",
    "    ],\n",
    "    [\n",
    "        [ 12,  13,  14,  15],\n",
    "        [ 16,  17, 18, 19],\n",
    "        [ 20, 21, 22, 23]\n",
    "    ]\n",
    "\n",
    "])\n",
    "print(f\"A.shape:{A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remind that alphabetical ordering leads the priority of the indices. \"ijk\" is different from \"jki\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div id='3d_identity'>Identity</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:07:15.591285Z",
     "start_time": "2021-01-06T17:07:15.586665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ijk', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:07:16.468785Z",
     "start_time": "2021-01-06T17:07:16.464175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ijk -> ijk', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transpositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swap axes *j* and *k*: axes transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:07:19.459132Z",
     "start_time": "2021-01-06T17:07:19.452458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  4.,  8.],\n",
       "         [ 1.,  5.,  9.],\n",
       "         [ 2.,  6., 10.],\n",
       "         [ 3.,  7., 11.]],\n",
       "\n",
       "        [[12., 16., 20.],\n",
       "         [13., 17., 21.],\n",
       "         [14., 18., 22.],\n",
       "         [15., 19., 23.]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i kj', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T21:48:57.537666Z",
     "start_time": "2021-01-03T21:48:57.524025Z"
    }
   },
   "source": [
    "##### Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.990498Z",
     "start_time": "2021-01-04T17:25:26.983234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 66., 210.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ijk->i', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:25:26.999129Z",
     "start_time": "2021-01-04T17:25:26.992938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 66., 210.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ijk ->i', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div id='3d_contraction'>Tensor contraction</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just another product against some axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:12:59.806702Z",
     "start_time": "2021-01-06T17:12:59.800525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B.shape:torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "B = torch.Tensor([\n",
    "    [\n",
    "        [ 10,  11,  12,  13],\n",
    "        [ 14,  15,  16,  17],\n",
    "        [ 18,  19, 20, 21]\n",
    "    ],\n",
    "    [\n",
    "        [ 32,  33,  34,  35],\n",
    "        [ 36,  37, 38, 39],\n",
    "        [ 40, 41, 42, 43]\n",
    "    ]\n",
    "\n",
    "])\n",
    "print(f\"B.shape:{B.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:13:57.287006Z",
     "start_time": "2021-01-06T17:13:57.284263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape:[2, 3, 4], B.shape:[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(f\"A.shape:{list(A.shape)}, B.shape:{list(B.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T17:14:27.837620Z",
     "start_time": "2021-01-06T17:14:27.832838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1960., 2020., 2080., 2140.],\n",
       "        [2110., 2176., 2242., 2308.],\n",
       "        [2260., 2332., 2404., 2476.],\n",
       "        [2410., 2488., 2566., 2644.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ijk, ijl->kl', A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div id='3d_batch_mat_prod'>Batch matrix multiplication</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T19:35:45.086700Z",
     "start_time": "2021-01-06T19:35:45.080596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape:[8, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(8*5).reshape(8,5)\n",
    "print(f\"A.shape:{list(A.shape)}\")\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T19:35:45.411758Z",
     "start_time": "2021-01-06T19:35:45.405209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B.shape:[5, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(5*3).reshape(5,3)\n",
    "print(f\"B.shape:{list(B.shape)}\")\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a normal matrix multiplication between A, (8,5) and B, (5,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T19:35:57.417581Z",
     "start_time": "2021-01-06T19:35:57.407996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  90,  100,  110],\n",
       "        [ 240,  275,  310],\n",
       "        [ 390,  450,  510],\n",
       "        [ 540,  625,  710],\n",
       "        [ 690,  800,  910],\n",
       "        [ 840,  975, 1110],\n",
       "        [ 990, 1150, 1310],\n",
       "        [1140, 1325, 1510]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split A in 4 chunks and compute the same multiplication as above, but batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T19:34:13.433274Z",
     "start_time": "2021-01-06T19:34:13.429352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29]],\n",
       "\n",
       "        [[30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_batched = A.reshape(4,2,5) # cut in four\n",
    "A_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T19:41:30.773296Z",
     "start_time": "2021-01-06T19:41:30.770050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_batched.shape:[4, 2, 5], B.shape:[5, 3]\n"
     ]
    }
   ],
   "source": [
    "print(f\"A_batched.shape:{list(A_batched.shape)}, B.shape:{list(B.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  90,  100,  110],\n",
       "         [ 240,  275,  310]],\n",
       "\n",
       "        [[ 390,  450,  510],\n",
       "         [ 540,  625,  710]],\n",
       "\n",
       "        [[ 690,  800,  910],\n",
       "         [ 840,  975, 1110]],\n",
       "\n",
       "        [[ 990, 1150, 1310],\n",
       "         [1140, 1325, 1510]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ijk, ku->iju', A_batched,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
